---
title: "HW5"
author: "Kaela Finegan and Lucas Shin"
date: "12/3/2020"
output: html_document
---

```{r}
library(png)
library(tidyverse)
library(stringr)
library(reshape)
library(ptw)
library(ggplot2)
```


Step 1: Run an all vs. all matching experiment on face data that we collected 
over the past five weeks (see Homework 4). In an all vs. all experiment, the 
enrolled/gallery and the query/probe set are the same. Therefore, you should 
have a symmetric matrix. 

All of Grant's code for HW4 implementing LBP:
```{r}
padding <- function(src){
  mat1 <- padzeros(src,1,side="both")
  mat2 <- padzeros(t(mat1),1,side="both") 
  return(t(mat2))
}

lbpthreshold <- function(mat){
  # Extracts the middle element of the 3 by 3 matrix
  val <- mat[2,2]
  tmp <- mat
  tmp[mat >= val] <- 1 
  tmp[mat < val] <- 0
  dst <- c(tmp[1,1], tmp[1,2], tmp[1,3], tmp[2,3], tmp[3,3], tmp[3,2], tmp[3,1], tmp[2,1])
  # Removes the middle element of the  3x3 vector
  return(dst) }

lbpval <- function(x){ val <- 0
for (i in 1:length(x)){
  val <- val + x[i] * 2 ^ (i-1)
}
return(val) }

# Creates a new matrix full of zeros, smaller in size to remove the zero padding
lbpmat <- function(src){
  dst <- matrix(0L, nrow = (dim(src)[1] - 2), ncol = (dim(src)[2] - 2))
  
  rows <- dim(src)[1] 
  cols <- dim(src)[2]
  for (row in c(2:(rows-1))){ 
    for(col in c(2:(cols-1))){
      tmp <- src[c( (row-1):(row+1)), c( (col-1):(col+1))] 
      threshvec <- lbpthreshold(tmp)
      binnum <- lbpval(threshvec)
      dst[row-1,col-1] <- binnum
    }
  }
  return(dst) 
}

lbphist <- function(src){
  bins <- seq(0,256,l=9)
  h <- hist(as.vector(src), breaks = bins,plot=FALSE) 
  return(h$counts)
}

lbpfeatvec <- function(src, numywin, numxwin){ 
  rows <- dim(src)[1]
  cols <- dim(src)[2]
  feats = c()
  yrange <- floor(rows / numywin) - 1 
  xrange <- floor(cols / numxwin) - 1
  for(y in c(1:numywin)){ 
    for(x in c(1:numxwin)){
      ymin <- y + (y-1)*yrange
      ymax <- y + (y-1)*yrange + yrange 
      xmin <- x + (x-1)*xrange
      xmax <- x + (x-1)*xrange + xrange
      tmp <- src[c(ymin:ymax),c(xmin:xmax)] 
      featvec <- lbphist(tmp)
      feats <- c(feats, featvec)
    } 
  }
  return(feats) }

lbpmatcher <- function(l1, l2){ 
  return (sum(abs(l1 - l2)))
}

imgdir <- "/Users/lucasshin/Desktop/Biometrics/Biometrics HW 5/fa20cropped/"
img_filenames <- list.files(path=imgdir, pattern="*.png")
# Stores all of the LBP feature vectors to compare two images
lbpimg <- vector("list", length(img_filenames)) 
lbpvecs <- vector("list", length(img_filenames))
# Creates 16 windows from image
nrows <- 4
ncols <- 4
for (i in 1:length(img_filenames)) {
  imgdata <- readPNG(paste(imgdir, img_filenames[i], sep =""))
  imgdata <- (imgdata - min(imgdata)) / (max(imgdata) - min(imgdata))
  paddedimg <- padding(imgdata)
  lbpimg[[i]] <- lbpmat(paddedimg)
  lbpvecs[[i]] <- lbpfeatvec(lbpimg[[i]], nrows, ncols)
}

#Create a matrix to hold scores
scores = matrix(0L, nrow = length(img_filenames), ncol = length(img_filenames)) 
for (i in 1:length(img_filenames)) {
for (j in 1:length(img_filenames)) {
scores[i,j] <- lbpmatcher(lbpvecs[[i]], lbpvecs[[j]])
} }

```
Step 1 for PCA:
```{r}
# Directory of Training Data
trainingdir <- "/Users/lucasshin/Desktop/Biometrics/Biometrics HW 5/testingfaces2/"
training_filenames <- list.files(path=trainingdir, pattern="*.png")
trainingdata <- vector("list", length(training_filenames))
for (i in 1:length(training_filenames)) {
trainingdata[[i]] <- readPNG(paste(trainingdir, training_filenames[i], sep =""))
trainingdata[[i]] <- (trainingdata[[i]] - min(trainingdata[[i]])) / (max(trainingdata[[i]]) -
                                                                       min(trainingdata[[i]])) 
}

trainingmatrix = c()
for (i in 1:length(training_filenames)){
  trainingmatrix = cbind(trainingmatrix, as.vector(t(trainingdata[[i]]))) 
}
u = rowMeans(trainingmatrix)

normmatrix <- apply(trainingmatrix, 2, function(x) x - u)

covarmtx = normmatrix %*% t(normmatrix)

ev = eigen(covarmtx)
pc = ev$vectors[,c(1:15)]


testingdir <- "/Users/lucasshin/Desktop/Biometrics/Biometrics HW 5/fa20cropped/"
testing_filenames <- list.files(path=testingdir, pattern="*.png")
testingdata <- vector("list", length(testing_filenames))
for (i in 1:length(testing_filenames)) {
testingdata[[i]] <- readPNG(paste(testingdir, testing_filenames[i], sep =""))
}
testingmatrix = c()
for (i in 1:length(testing_filenames)){
  testingmatrix = cbind(testingmatrix, as.vector(t(testingdata[[i]]))) 
}


weights <- vector("list", 0)
for(img in 1:length(testing_filenames)){
  img_data <- testingmatrix[ , img]
  weight <- t(pc) %*% img_data
  weight2 <- abs(weight)
  #weights <- c(weights, weight2)
  weights[[img]] <- weight2
}

#Create a matrix to hold scores
scores1 = matrix(0L, nrow = length(testing_filenames), ncol = length(testing_filenames)) 
for (i in 1:length(testing_filenames)) {
  for (j in 1:length(testing_filenames)) {
    scores1[i,j] <- lbpmatcher(weights[[i]], weights[[j]])
} }
```



Step 2: Plot the distribution of genuine and impostor scores 
for the LBP and PCA face matching algorithms (see Homework 1).
```{r}
d <- scores
gallery <- c()
query <- c()

for (i in 1:length(img_filenames)) {
   gallery <- c(gallery, substr(img_filenames[i], start = 2, stop = 4))
}

gallery <- as.matrix(gallery)
query <- gallery

colnames(d) <- as.matrix(gallery) 
rownames(d) <- as.matrix(query)
m <- melt(d)
names(m) <- c("xA", "xE", "dist")

genuine <- m[m[,1] == m[,2],] 
impostor <- m[m[,1] != m[,2],]

genden <- density(genuine[,3]) 
impden <- density(impostor[,3])

xmin <- min(genden$x,impden$x) 
xmax <- max(genden$x,impden$x) 
ymin <- min(genden$y,impden$y) 
ymax <- max(genden$y,impden$y)

plot(genden, xlim=c(xmin,xmax), ylim=c(ymin,ymax), main="Distribution of Genuine and Impostor Scores (LBP before Normalization)",
     xlab="Scores", ylab="Density", col = "blue", lwd = 2)
lines(impden, col = "red", lwd = 2)
polygon(genden, col=rgb(0,0,1,0.5))
polygon(impden, col=rgb(1,0,0,0.5))
legend('topright', c("Genuine","Impostor"), col = c("blue","red"), lty = c(1,1), lwd = c(2.5,2.5))

dprime = function(mu1,mu0,sig1,sig0) { sqrt(2) * abs(mu1 - mu0) / sqrt(sig1^2 + sig0^2) } 
mu1 = mean(genuine[,3])
mu0 = mean(impostor[,3])
sig1 = sd(genuine[,3])
sig0 = sd(impostor[,3])
#print(paste("Sensitivity Index:",dprime(mu1,mu0,sig1,sig0)))

# Distance scores are used to compute this DET curve. If the scores are # similarity scores, then reverse the logical operators
DET = function(threshold, gen, imp){
    # The percentage of impostor scores that are claimed to be genuine
  far = length(imp[ imp < threshold]) / length(imp)
  # The percentage of true identities that are claimed to be impostors 
  frr = length(gen[ gen > threshold]) / length(gen)
  datapoint = c(threshold,far,frr) 
  return(datapoint)
}

thresholds = seq(xmin, xmax,(xmax-xmin)/100)
detcurve = t(sapply(thresholds, DET, genuine[,3], impostor[,3])) 
colnames(detcurve) = c("threshold","far","frr")
#plot(detcurve[,2], detcurve[,3], type = "l", lwd = 2, xlab = "FAR", ylab = "FRR", main="Detection Error Tradeoff #Curve (LBP before Normalization)")
#y = seq(0,1)
#x = seq(0,1)
#lines(x,y , lty = 2, col = "gray40")
#legend('topright', c("ROC","ERR"), col = c("black","gray40"), lty = c(1,2), lwd = c(2.5,2.5))

```
Step 2 for PCA:
```{r}
d <- scores1

names <- c()
for(image in 1:length(testing_filenames)){
  id <- str_sub(testing_filenames[image], 2, 4)
  names <- c(names, id)
}

query <- as.matrix(names)
gallery <-  as.matrix(names)
colnames(d) <- gallery
rownames(d) <- query

m <- melt(d)
names(m) <- c("xA", "xE", "dist")

genuine <- m[m[,1] == m[,2],] 
impostor <- m[m[,1] != m[,2],]

genden <- density(genuine[,3]) 
impden <- density(impostor[,3])

xmin <- min(genden$x,impden$x) 
xmax <- max(genden$x,impden$x) 
ymin <- min(genden$y,impden$y) 
ymax <- max(genden$y,impden$y)


plot(genden, xlim=c(xmin,xmax), ylim=c(ymin,ymax), main="Distribution of Genuine and Impostor Scores (PCA before Normalization)",
     xlab="Scores", ylab="Density", col = "blue", lwd = 2)
lines(impden, col = "red", lwd = 2)
polygon(genden, col=rgb(0,0,1,0.5))
polygon(impden, col=rgb(1,0,0,0.5))
legend('topright', c("Genuine","Impostor"), col = c("blue","red"), lty = c(1,1), lwd = c(2.5,2.5))

dprime = function(mu1,mu0,sig1,sig0) { sqrt(2) * abs(mu1 - mu0) / sqrt(sig1^2 + sig0^2) } 
mu1 = mean(genuine[,3])
mu0 = mean(impostor[,3])
sig1 = sd(genuine[,3])
sig0 = sd(impostor[,3])
#print(paste("Sensitivity Index:",dprime(mu1,mu0,sig1,sig0)))

# Distance scores are used to compute this DET curve. If the scores are # similarity scores, then reverse the logical operators
DET = function(threshold, gen, imp){
    # The percentage of impostor scores that are claimed to be genuine
  far = length(imp[ imp < threshold]) / length(imp)
  # The percentage of true identities that are claimed to be impostors 
  frr = length(gen[ gen > threshold]) / length(gen)
  datapoint = c(threshold,far,frr) 
  return(datapoint)
}

thresholds = seq(xmin, xmax,(xmax-xmin)/100)
detcurve = t(sapply(thresholds, DET, genuine[,3], impostor[,3])) 
colnames(detcurve) = c("threshold","far","frr")
# plot(detcurve[,2], detcurve[,3], type = "l", lwd = 2, xlab = "FAR", ylab = "FRR", main="Detection Error #Tradeoff Curve (PCA before Normalization)")
# y = seq(0,1)
# x = seq(0,1)
# lines(x,y , lty = 2, col = "gray40")
# legend('topright', c("ROC","ERR"), col = c("black","gray40"), lty = c(1,2), lwd = c(2.5,2.5))
```
Step 3: Score Normalization
```{r}
scores_lbp <- scores
scores_pca <- scores1

max_lbp <- max(scores_lbp)
max_pca <- max(scores_pca)
min_lbp <- min(scores_lbp)
min_pca <- min(scores_pca)
range_lbp <- max_lbp - min_lbp
range_pca <- max_pca - min_pca


for (i in 1:length(img_filenames)) {
  for (j in 1:length(img_filenames)) {
    scores_lbp[i,j] <- (scores_lbp[i,j] - min_lbp) / range_lbp
  } }

for (i in 1:length(testing_filenames)) {
  for (j in 1:length(testing_filenames)) {
    scores_pca[i,j] <- (scores_pca[i,j] - min_pca) / range_pca
} }

```
Step 4: Score Fusion
```{r}
#Create a matrix to hold scores

scores_max = matrix(0L, nrow = length(testing_filenames), ncol = length(testing_filenames)) 
for (i in 1:length(testing_filenames)) {
  for (j in 1:length(testing_filenames)) {
    scores_max[i,j] <- max(scores_pca[i,j], scores_lbp[i,j])
  } }

scores_min = matrix(0L, nrow = length(testing_filenames), ncol = length(testing_filenames)) 
for (i in 1:length(testing_filenames)) {
  for (j in 1:length(testing_filenames)) {
    scores_min[i,j] <- min(scores_pca[i,j], scores_lbp[i,j])
  } }

scores_sum = matrix(0L, nrow = length(testing_filenames), ncol = length(testing_filenames)) 
for (i in 1:length(testing_filenames)) {
  for (j in 1:length(testing_filenames)) {
    scores_sum[i,j] <- scores_pca[i,j] + scores_lbp[i,j]
  } }
```
Step 5: Make ROC curve for scores, scores1, scores_min, scores_max, scores_sum and 
Make Distribution graph for each of them
```{r}
# for scores
d <- scores_lbp

names <- c()
for(image in 1:length(img_filenames)){
  id <- str_sub(img_filenames[image], 2, 4)
  names <- c(names, id)
}

query <- as.matrix(names)
gallery <-  as.matrix(names)
colnames(d) <- gallery
rownames(d) <- query

m <- melt(d)
names(m) <- c("xA", "xE", "dist")

genuine <- m[m[,1] == m[,2],] 
impostor <- m[m[,1] != m[,2],]

genden <- density(genuine[,3]) 
impden <- density(impostor[,3])

xmin <- min(genden$x,impden$x) 
xmax <- max(genden$x,impden$x) 
ymin <- min(genden$y,impden$y) 
ymax <- max(genden$y,impden$y)

plot(genden, xlim=c(xmin,xmax), ylim=c(ymin,ymax), main="Distribution of Genuine and Impostor Scores (LBP after Normalization)",
     xlab="Scores", ylab="Density", col = "blue", lwd = 2)
lines(impden, col = "red", lwd = 2)
polygon(genden, col=rgb(0,0,1,0.5))
polygon(impden, col=rgb(1,0,0,0.5))
legend('topright', c("Genuine","Impostor"), col = c("blue","red"), lty = c(1,1), lwd = c(2.5,2.5))

dprime = function(mu1,mu0,sig1,sig0) { sqrt(2) * abs(mu1 - mu0) / sqrt(sig1^2 + sig0^2) } 
mu1 = mean(genuine[,3])
mu0 = mean(impostor[,3])
sig1 = sd(genuine[,3])
sig0 = sd(impostor[,3])
#print(paste("Sensitivity Index:",dprime(mu1,mu0,sig1,sig0)))

# Distance scores are used to compute this DET curve. If the scores are # similarity scores, then reverse the logical operators
DET = function(threshold, gen, imp){
    # The percentage of impostor scores that are claimed to be genuine
  far = length(imp[ imp < threshold]) / length(imp)
  # The percentage of true identities that are claimed to be impostors 
  frr = length(gen[ gen > threshold]) / length(gen)
  datapoint = c(threshold,far,frr) 
  return(datapoint)
}

thresholds = seq(xmin, xmax,(xmax-xmin)/100)
detcurve1 = t(sapply(thresholds, DET, genuine[,3], impostor[,3])) 
colnames(detcurve1) = c("threshold","far","frr")

# for scores1
d <- scores_pca

names <- c()
for(image in 1:length(img_filenames)){
  id <- str_sub(img_filenames[image], 2, 4)
  names <- c(names, id)
}

query <- as.matrix(names)
gallery <-  as.matrix(names)
colnames(d) <- gallery
rownames(d) <- query

m <- melt(d)
names(m) <- c("xA", "xE", "dist")

genuine <- m[m[,1] == m[,2],] 
impostor <- m[m[,1] != m[,2],]

genden <- density(genuine[,3]) 
impden <- density(impostor[,3])

xmin <- min(genden$x,impden$x) 
xmax <- max(genden$x,impden$x) 
ymin <- min(genden$y,impden$y) 
ymax <- max(genden$y,impden$y)

plot(genden, xlim=c(xmin,xmax), ylim=c(ymin,ymax), main="Distribution of Genuine and Impostor Scores (PCA after Normalization)",
     xlab="Scores", ylab="Density", col = "blue", lwd = 2)
lines(impden, col = "red", lwd = 2)
polygon(genden, col=rgb(0,0,1,0.5))
polygon(impden, col=rgb(1,0,0,0.5))
legend('topright', c("Genuine","Impostor"), col = c("blue","red"), lty = c(1,1), lwd = c(2.5,2.5))

dprime = function(mu1,mu0,sig1,sig0) { sqrt(2) * abs(mu1 - mu0) / sqrt(sig1^2 + sig0^2) } 
mu1 = mean(genuine[,3])
mu0 = mean(impostor[,3])
sig1 = sd(genuine[,3])
sig0 = sd(impostor[,3])
#print(paste("Sensitivity Index:",dprime(mu1,mu0,sig1,sig0)))

# Distance scores are used to compute this DET curve. If the scores are # similarity scores, then reverse the logical operators
DET = function(threshold, gen, imp){
    # The percentage of impostor scores that are claimed to be genuine
  far = length(imp[ imp < threshold]) / length(imp)
  # The percentage of true identities that are claimed to be impostors 
  frr = length(gen[ gen > threshold]) / length(gen)
  datapoint = c(threshold,far,frr) 
  return(datapoint)
}

thresholds = seq(xmin, xmax,(xmax-xmin)/100)
detcurve2 = t(sapply(thresholds, DET, genuine[,3], impostor[,3])) 
colnames(detcurve2) = c("threshold","far","frr")


# scores_max
d <- scores_max

names <- c()
for(image in 1:length(img_filenames)){
  id <- str_sub(img_filenames[image], 2, 4)
  names <- c(names, id)
}

query <- as.matrix(names)
gallery <-  as.matrix(names)
colnames(d) <- gallery
rownames(d) <- query

m <- melt(d)
names(m) <- c("xA", "xE", "dist")

genuine <- m[m[,1] == m[,2],] 
impostor <- m[m[,1] != m[,2],]

genden <- density(genuine[,3]) 
impden <- density(impostor[,3])

xmin <- min(genden$x,impden$x) 
xmax <- max(genden$x,impden$x) 
ymin <- min(genden$y,impden$y) 
ymax <- max(genden$y,impden$y)

plot(genden, xlim=c(xmin,xmax), ylim=c(ymin,ymax), main="Distribution of Genuine and Impostor Scores (Maximum Fusion)",
     xlab="Scores", ylab="Density", col = "blue", lwd = 2)
lines(impden, col = "red", lwd = 2)
polygon(genden, col=rgb(0,0,1,0.5))
polygon(impden, col=rgb(1,0,0,0.5))
legend('topright', c("Genuine","Impostor"), col = c("blue","red"), lty = c(1,1), lwd = c(2.5,2.5))

dprime = function(mu1,mu0,sig1,sig0) { sqrt(2) * abs(mu1 - mu0) / sqrt(sig1^2 + sig0^2) } 
mu1 = mean(genuine[,3])
mu0 = mean(impostor[,3])
sig1 = sd(genuine[,3])
sig0 = sd(impostor[,3])
#print(paste("Sensitivity Index:",dprime(mu1,mu0,sig1,sig0)))

# Distance scores are used to compute this DET curve. If the scores are # similarity scores, then reverse the logical operators
DET = function(threshold, gen, imp){
    # The percentage of impostor scores that are claimed to be genuine
  far = length(imp[ imp < threshold]) / length(imp)
  # The percentage of true identities that are claimed to be impostors 
  frr = length(gen[ gen > threshold]) / length(gen)
  datapoint = c(threshold,far,frr) 
  return(datapoint)
}

thresholds = seq(xmin, xmax,(xmax-xmin)/100)
detcurve3 = t(sapply(thresholds, DET, genuine[,3], impostor[,3])) 
colnames(detcurve3) = c("threshold","far","frr")

# for scores_min
d <- scores_min

names <- c()
for(image in 1:length(img_filenames)){
  id <- str_sub(img_filenames[image], 2, 4)
  names <- c(names, id)
}

query <- as.matrix(names)
gallery <-  as.matrix(names)
colnames(d) <- gallery
rownames(d) <- query

m <- melt(d)
names(m) <- c("xA", "xE", "dist")

genuine <- m[m[,1] == m[,2],] 
impostor <- m[m[,1] != m[,2],]

genden <- density(genuine[,3]) 
impden <- density(impostor[,3])

xmin <- min(genden$x,impden$x) 
xmax <- max(genden$x,impden$x) 
ymin <- min(genden$y,impden$y) 
ymax <- max(genden$y,impden$y)

plot(genden, xlim=c(xmin,xmax), ylim=c(ymin,ymax), main="Distribution of Genuine and Impostor Scores (Minimum Fusion)",
     xlab="Scores", ylab="Density", col = "blue", lwd = 2)
lines(impden, col = "red", lwd = 2)
polygon(genden, col=rgb(0,0,1,0.5))
polygon(impden, col=rgb(1,0,0,0.5))
legend('topright', c("Genuine","Impostor"), col = c("blue","red"), lty = c(1,1), lwd = c(2.5,2.5))

dprime = function(mu1,mu0,sig1,sig0) { sqrt(2) * abs(mu1 - mu0) / sqrt(sig1^2 + sig0^2) } 
mu1 = mean(genuine[,3])
mu0 = mean(impostor[,3])
sig1 = sd(genuine[,3])
sig0 = sd(impostor[,3])
#print(paste("Sensitivity Index:",dprime(mu1,mu0,sig1,sig0)))

# Distance scores are used to compute this DET curve. If the scores are # similarity scores, then reverse the logical operators
DET = function(threshold, gen, imp){
    # The percentage of impostor scores that are claimed to be genuine
  far = length(imp[ imp < threshold]) / length(imp)
  # The percentage of true identities that are claimed to be impostors 
  frr = length(gen[ gen > threshold]) / length(gen)
  datapoint = c(threshold,far,frr) 
  return(datapoint)
}

thresholds = seq(xmin, xmax,(xmax-xmin)/100)
detcurve4 = t(sapply(thresholds, DET, genuine[,3], impostor[,3])) 
colnames(detcurve4) = c("threshold","far","frr")


# for scores_sum
d <- scores_sum

names <- c()
for(image in 1:length(img_filenames)){
  id <- str_sub(img_filenames[image], 2, 4)
  names <- c(names, id)
}

query <- as.matrix(names)
gallery <-  as.matrix(names)
colnames(d) <- gallery
rownames(d) <- query

m <- melt(d)
names(m) <- c("xA", "xE", "dist")

genuine <- m[m[,1] == m[,2],] 
impostor <- m[m[,1] != m[,2],]

genden <- density(genuine[,3]) 
impden <- density(impostor[,3])

xmin <- min(genden$x,impden$x) 
xmax <- max(genden$x,impden$x) 
ymin <- min(genden$y,impden$y) 
ymax <- max(genden$y,impden$y)

plot(genden, xlim=c(xmin,xmax), ylim=c(ymin,ymax), main="Distribution of Genuine and Impostor Scores (Sum Fusion)",
     xlab="Scores", ylab="Density", col = "blue", lwd = 2)
lines(impden, col = "red", lwd = 2)
polygon(genden, col=rgb(0,0,1,0.5))
polygon(impden, col=rgb(1,0,0,0.5))
legend('topright', c("Genuine","Impostor"), col = c("blue","red"), lty = c(1,1), lwd = c(2.5,2.5))

dprime = function(mu1,mu0,sig1,sig0) { sqrt(2) * abs(mu1 - mu0) / sqrt(sig1^2 + sig0^2) } 
mu1 = mean(genuine[,3])
mu0 = mean(impostor[,3])
sig1 = sd(genuine[,3])
sig0 = sd(impostor[,3])
#print(paste("Sensitivity Index:",dprime(mu1,mu0,sig1,sig0)))

# Distance scores are used to compute this DET curve. If the scores are # similarity scores, then reverse the logical operators
DET = function(threshold, gen, imp){
    # The percentage of impostor scores that are claimed to be genuine
  far = length(imp[ imp < threshold]) / length(imp)
  # The percentage of true identities that are claimed to be impostors 
  frr = length(gen[ gen > threshold]) / length(gen)
  datapoint = c(threshold,far,frr) 
  return(datapoint)
}

thresholds = seq(xmin, xmax,(xmax-xmin)/100)
detcurve5 = t(sapply(thresholds, DET, genuine[,3], impostor[,3])) 
colnames(detcurve5) = c("threshold","far","frr")






plot(detcurve1[,2], detcurve1[,3], type = "l", lwd = 2, xlab = "FAR", ylab = "FRR", main="ROC Curves After Normalizaion")
lines(detcurve2[,2], detcurve2[,3], col = "blue")
lines(detcurve3[,2], detcurve3[,3], col = "red")
lines(detcurve4[,2], detcurve4[,3], col = "green")
lines(detcurve5[,2], detcurve5[,3], col = "orange")
legend('topright', c("LBP","PCA", "Max", "Min", "Sum"), col = c("black","blue", "red", "green", "orange"), lty = 1, lwd = c(2.5,2.5))
```

Qualitative assessment of 3-4 genuine pairs and 1-2 impostor pairs:
For this step I am choosing to use the sum fusion of LBP and PCA after 
score normalization.

Genuine Pair One
```{r}
# row 43 column 42 in the score matrix for sum fusion have a score of 0.3266709434
person1 <- testing_filenames[43]
person2 <- testing_filenames[42]
genuine_score1 <- scores_sum[43, 42]
paste(person1, " and ", person2, " are a genuine pair with a score of ", genuine_score1, ". This is because the person in each of those pictures has generally the same facial expression, lighting and angle of the face. They appear to be in the same location, and the images are cropped in the same manner. This explains their low distance score.", sep = "")
```

Genuine Pair Two
```{r}
# row 157 column 174 in the score matrix for sum fusion have a score of 0.2885097193
person3 <- testing_filenames[167]
person4 <- testing_filenames[174]
genuine_score2 <- scores_sum[167, 174]
paste(person3, " and ", person4, " are a genuine pair with a score of ", genuine_score2, ". This is because the person in each of those pictures has generally the same facial expression, lighting and angle of the face. They have long hair which takes up a lot of frame that would normally be the background. Also the lighting is on the right side of both images creating the same shadows. This explains their low distance score.", sep = "")
```

Genuine Pair Three
```{r}
# row 66 column 67 in the score matrix for sum fusion have a score of 0.2600269827
person5 <- testing_filenames[66]
person6 <- testing_filenames[67]
genuine_score3 <- scores_sum[66, 67]
paste(person3, " and ", person4, " are a genuine pair with a score of ", genuine_score2, ". This is because the person in each of those pictures has long hair, thick facial hair and glasses. Those dark features obscure the fact that he is making different facial expressions. Combined with very similar lighting, the images appear to be the same. This explains their low distance score.", sep = "")
```
Genuine Pair Four
```{r}
# row 18 column 17 in the score matrix for sum fusion have a score of 1.393449983
person8 <- testing_filenames[18]
person9 <- testing_filenames[17]
genuine_score4 <- scores_sum[18, 17]
paste(person8, " and ", person9, " are a genuine pair with a score of ", genuine_score4, ". This is because the the first photo is in dark lighting and the second photo is in bright lighting. The angle of the first photo is straight on and the angle of the second photo is tilted. Also the background is light in the first and dark in the second. This explains the high distance score.", sep = "")
```

Impostor Pair One
```{r}
# row 14 column 92 in the score matrix for sum fusion have a score of 1.685795738
person7 <- testing_filenames[14]
person8 <- testing_filenames[92]
imposter_score1 <- scores_sum[14, 92]
paste(person7, " and ", person8, " are an impostor pair with a score of ", imposter_score1, ". This is because the face in the photo ", person7, " has really dark lighting and a light background, while the face in the photo ", person8, " has bright lighting and a dark background filled with their hair. This compared with their very different facial structures explains the high distance score.", sep = "")
```
Impostor Pair Two
```{r}
# row 88 column 60 in the score matrix for sum fusion have a score of 0.4923824218
person10 <- testing_filenames[88]
person11 <- testing_filenames[60]
imposter_score2 <- scores_sum[88, 60]
paste(person10, " and ", person11, " are an impostor pair with a score of ", imposter_score2, ". This is because the person in the photo ", person10, " has a really light background, while the person in the photo ", person11, " long blond hair acting as a light background. The person in the photo", person10, " is tilting their head, while the person in photo ", person11, " is looking straight on. This is mask their differences in their facial structures. This explains the low distance score.", sep = "")
```
